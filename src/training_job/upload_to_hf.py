import gcsfs
import numpy as np
from datasets import Dataset, Features, Array2D, ClassLabel, Value

# --- CONFIGURATION ---
GCS_PROJECT_ID = 'early-exit-transformer-network'
BUCKET_PATH = "encoder-models-2/siebert-data/siebert-actual-data/core_0" # The folder where inference.py saved the chunks
HF_REPO_ID = "mxi71/twitter-100m-siebert-activations"
# ---------------------

# 1. Setup GCS Connection
fs = gcsfs.GCSFileSystem(project=GCS_PROJECT_ID)

# Find all .npz files (generated by your workers)
file_paths = fs.glob(f"{BUCKET_PATH}/*.npz")
print(f"Found {len(file_paths)} npz files in GCS.")

if not file_paths:
    raise ValueError("No .npz files found. Check your BUCKET_PATH.")

# 2. Inspect the first file to determine Hidden Dimension dynamically
# We know the middle dim is 25 (EXPECTED_CLS_COUNT), but we need the Hidden Size (e.g., 768 or 1024)
with fs.open(file_paths[0], 'rb') as f:
    sample_data = np.load(f)
    print(f"Keys in npz file: {list(sample_data.keys())}")
    
    # Heuristic: The 3D array is the CLS tokens, the 1D array is the labels
    # Adjust 'arr_0' / 'arr_1' if your inference.py used specific names like 'cls_tokens'
    keys = list(sample_data.keys())
    
    # Assuming arr_0 is CLS and arr_1 is Labels based on your main2.py order
    cls_key = keys[0] 
    label_key = keys[1]
    
    # Get shape: (Batch, 25, Hidden)
    sample_shape = sample_data[cls_key].shape 
    hidden_dim = sample_shape[2] 
    print(f"Detected shape: {sample_shape}. Hidden Dimension: {hidden_dim}")

# 3. Define the Generator
def data_generator():
    for file_path in file_paths:
        try:
            with fs.open(file_path, 'rb') as f:
                data = np.load(f)
                
                # Load the full arrays for this chunk into memory
                cls_tokens_chunk = data[cls_key]   # Shape: (N, 25, Hidden)
                labels_chunk = data[label_key]     # Shape: (N,)
                
                # Iterate over the batch dimension (N) to yield individual samples
                for i in range(len(cls_tokens_chunk)):
                    yield {
                        "cls_tokens": cls_tokens_chunk[i], # Yields (25, Hidden)
                        "label": int(labels_chunk[i]),     # Yields integer 0 or 1
                        "origin_file": file_path.split("/")[-1] # Useful for debugging
                    }
        except Exception as e:
            print(f"Error reading {file_path}: {e}")

# 4. Define Features
# Explicitly telling HF the shape prevents it from guessing wrong
features = Features({
    "cls_tokens": Array2D(shape=(25, hidden_dim), dtype="float32"),
    "label": ClassLabel(num_classes=2, names=["0", "1"]),
    "origin_file": Value("string")
})

# 5. Create and Push
print("Generatng dataset...")
dataset = Dataset.from_generator(data_generator, features=features)

print("Pushing to Hub...")
dataset.push_to_hub(HF_REPO_ID)
print("Done!")